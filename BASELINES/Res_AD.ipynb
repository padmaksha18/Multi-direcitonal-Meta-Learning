{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1xbqy52vhSL",
        "outputId": "5f6b1914-f4ab-47b5-802c-82908a2c519c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/DATASETS/CSE_CIC_IDS_ALL_DATA/CSE-CIC-IDS2018/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGa9WKn7vjFk",
        "outputId": "144fd3ad-5b25-48dd-dd3b-c2e7aa5c3f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/DATASETS/CSE_CIC_IDS_ALL_DATA/CSE-CIC-IDS2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c06hcOKtvjHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MjJgECgZvjKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ResAD — MICRO PILOT (minutes to first results)\n",
        "# -----------------------------------------------\n",
        "# Design goals\n",
        "#   • Use **all families** but with a **tiny, fixed sample per class per split** (defaults = 40/40).\n",
        "#   • No nearest-neighbor search → **centroid residuals** per family.\n",
        "#   • No flow → **diagonal Gaussian** on constrained residuals.\n",
        "#   • **Zero AE pretraining by default** (can toggle to 1 epoch).\n",
        "#   • Optional **feature cap**: keep top-K-variance numeric columns (defaults K=32).\n",
        "#   • Prints overall + per-family metrics at VAL and TEST.\n",
        "#\n",
        "# Expectation: Should finish in a couple of minutes even on CPU.\n",
        "\n",
        "import os, re, argparse, random, hashlib, math\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
        "\n",
        "# ---------------- File discovery ----------------\n",
        "\n",
        "def resolve_data_dir(path: str) -> str:\n",
        "    cand = os.path.expanduser(path.rstrip(\"/ \"))\n",
        "    if os.path.isdir(cand): return cand\n",
        "    variants = [cand.replace(\"CSE-CIC-IDS_ALL_DATA\",\"CSE_CIC_IDS_ALL_DATA\"), cand.replace(\"CSE_CIC_IDS_ALL_DATA\",\"CSE-CIC-IDS_ALL_DATA\")]\n",
        "    for v in variants:\n",
        "        if os.path.isdir(v): return v\n",
        "    raise FileNotFoundError(f\"Directory not found: {path}\\nTried: {cand}\")\n",
        "\n",
        "def _clean_display_name(name: str) -> str:\n",
        "    s = name.strip();\n",
        "    if len(s)>=2 and s[0]==s[-1] and s[0] in (\"'\",'\"'): s = s[1:-1]\n",
        "    return s\n",
        "\n",
        "def _parse_cat_subcat_from_display_name(clean_name: str) -> Tuple[str,str]:\n",
        "    parts = re.split(r\"[-–—_]\", clean_name, maxsplit=1)\n",
        "    if len(parts)==2: return parts[0].strip(), parts[1].strip()\n",
        "    return clean_name.strip(), \"\"\n",
        "\n",
        "def discover_files_by_category(data_dir: str) -> Dict[str,str]:\n",
        "    data_dir = resolve_data_dir(data_dir)\n",
        "    out = {}\n",
        "    with os.scandir(data_dir) as it:\n",
        "        for e in it:\n",
        "            if e.is_file() and e.name.lower().endswith('.csv'):\n",
        "                cleaned = _clean_display_name(os.path.splitext(e.name)[0])\n",
        "                cat, sub = _parse_cat_subcat_from_display_name(cleaned)\n",
        "                fam = f\"{cat}::{sub}\" if sub else cat\n",
        "                out[fam] = e.path\n",
        "    if not out: raise RuntimeError(f\"No CSVs found in {data_dir}\")\n",
        "    return out\n",
        "\n",
        "# ---------------- Labels & split policy ----------------\n",
        "\n",
        "LABEL_CANDIDATES = [\"Label\",\"label\",\"Attack\",\"attack\",\"is_anomaly\",\"anomaly\",\"Anomaly\",\"benign_label\"]\n",
        "BENIGN_TOKENS = {\"BENIGN\",\"Benign\",\"benign\",\"Normal\",\"normal\",\"0\",0,0.0}\n",
        "ATTACK_TOKENS = {\"ATTACK\",\"Attack\",\"attack\",\"1\",1,1.0}\n",
        "\n",
        "def infer_label_column(df: pd.DataFrame) -> str:\n",
        "    for c in LABEL_CANDIDATES:\n",
        "        if c in df.columns: return c\n",
        "    for c in df.columns:\n",
        "        lc=c.lower();\n",
        "        if lc.endswith('label') or lc.endswith('attack') or lc.endswith('anomaly'):\n",
        "            return c\n",
        "    raise KeyError(\"Could not infer label column\")\n",
        "\n",
        "def to_binary_series(raw: pd.Series) -> pd.Series:\n",
        "    if np.issubdtype(raw.dtype,np.number): return (raw.astype(float)>0.0).astype(int)\n",
        "    s = raw.astype(str)\n",
        "    return s.apply(lambda v: 0 if v in {str(t) for t in BENIGN_TOKENS} else (1 if v in {str(t) for t in ATTACK_TOKENS} else (1 if v.strip().lower() not in (\"benign\",\"normal\",\"0\") else 0)))\n",
        "\n",
        "@dataclass\n",
        "class SplitPolicy:\n",
        "    train_frac: float = 2/3\n",
        "    val_frac: float = 1/6\n",
        "    seed: int = 42\n",
        "    def assign(self, file_key: str, row_idx: int) -> str:\n",
        "        h = hashlib.blake2b(digest_size=8)\n",
        "        h.update(str(self.seed).encode()); h.update(file_key.encode()); h.update(str(row_idx).encode())\n",
        "        u = int.from_bytes(h.digest(),'little')/2**64\n",
        "        if u < self.train_frac: return 'train'\n",
        "        if u < self.train_frac + self.val_frac: return 'val'\n",
        "        return 'test'\n",
        "\n",
        "# ---------------- Online scaler ----------------\n",
        "\n",
        "class OnlineStandardizer:\n",
        "    def __init__(self, dim:int):\n",
        "        self.n=0; self.mean=np.zeros(dim,np.float64); self.M2=np.zeros(dim,np.float64)\n",
        "        self.std=np.ones(dim,np.float64)\n",
        "    def partial_fit(self, X: np.ndarray):\n",
        "        X=np.atleast_2d(X).astype(np.float64)\n",
        "        for x in X:\n",
        "            self.n+=1; d=x-self.mean; self.mean+=d/self.n; self.M2+=d*(x-self.mean)\n",
        "    def finalize(self):\n",
        "        var=self.M2/max(1,self.n-1); self.std=np.sqrt(np.maximum(var,1e-8)).astype(np.float32); self.mean=self.mean.astype(np.float32); return self\n",
        "    def transform(self, X: np.ndarray)->np.ndarray:\n",
        "        X=X.astype(np.float32); return (X-self.mean)/self.std\n",
        "\n",
        "# ---------------- Feature capping (top-K variance) ----------------\n",
        "\n",
        "def topk_variance_columns(df: pd.DataFrame, exclude: List[str], k: int) -> List[str]:\n",
        "    num = df.select_dtypes(include=[np.number])\n",
        "    cols = [c for c in num.columns if c not in exclude]\n",
        "    if 0 < k < len(cols):\n",
        "        v = num[cols].var().sort_values(ascending=False)\n",
        "        return v.index[:k].tolist()\n",
        "    return cols\n",
        "\n",
        "# ---------------- Pilot sampler (tiny quotas) ----------------\n",
        "\n",
        "def build_micro_pilot(files, fam_names, numeric_cols, label_col, policy: SplitPolicy, chunk_rows:int,\n",
        "                      per_fam_norm:int, per_fam_anom:int):\n",
        "    quotas={'train':{'norm':per_fam_norm,'anom':per_fam_anom},\n",
        "            'val':  {'norm':per_fam_norm,'anom':per_fam_anom},\n",
        "            'test': {'norm':per_fam_norm,'anom':per_fam_anom}}\n",
        "    buffers={s:{'X':[], 'y':[], 'fam':[]} for s in ('train','val','test')}\n",
        "    scaler=OnlineStandardizer(dim=len(numeric_cols))\n",
        "\n",
        "    def under_quota(split,fid,y):\n",
        "        kind='anom' if y==1 else 'norm'\n",
        "        c=0\n",
        "        for yy,ff in zip(buffers[split]['y'], buffers[split]['fam']):\n",
        "            if ff==fid and ((yy==1)==(kind=='anom')): c+=1\n",
        "        return c < quotas[split][kind]\n",
        "\n",
        "    # Go family by family, fill tiny quotas and move on.\n",
        "    for fid,fname in enumerate(fam_names):\n",
        "        path = files[fname]\n",
        "        for chunk in pd.read_csv(path, chunksize=chunk_rows):\n",
        "            lbl = label_col if label_col in chunk.columns else infer_label_column(chunk)\n",
        "            y = to_binary_series(chunk[lbl])\n",
        "            X = chunk[numeric_cols].copy().replace([np.inf,-np.inf],np.nan).dropna(axis=0)\n",
        "            if len(X)==0: continue\n",
        "            y = y.loc[X.index]\n",
        "            for local_idx,row_idx in enumerate(X.index):\n",
        "                split = policy.assign(fname, int(row_idx))\n",
        "                yi = int(y.loc[row_idx])\n",
        "                if not under_quota(split,fid,yi):\n",
        "                    continue\n",
        "                xi = X.iloc[local_idx].values.astype(np.float32)\n",
        "                if split=='train' and yi==0: scaler.partial_fit(xi[None,:])\n",
        "                buffers[split]['X'].append(xi); buffers[split]['y'].append(yi); buffers[split]['fam'].append(fid)\n",
        "            # Stop early if this family's quotas are full for all splits\n",
        "            done_all=True\n",
        "            for s in ('train','val','test'):\n",
        "                for kind in ('norm','anom'):\n",
        "                    want=quotas[s][kind]\n",
        "                    have=sum(1 for yy,ff in zip(buffers[s]['y'], buffers[s]['fam']) if ff==fid and ((yy==1)==(kind=='anom')))\n",
        "                    if have<want: done_all=False; break\n",
        "                if not done_all: break\n",
        "            if done_all: break\n",
        "    scaler.finalize()\n",
        "\n",
        "    def pack(split):\n",
        "        X=np.array(buffers[split]['X'],np.float32)\n",
        "        if X.size: X=scaler.transform(X)\n",
        "        y=np.array(buffers[split]['y'],np.int64)\n",
        "        f=np.array(buffers[split]['fam'],np.int64)\n",
        "        return {'X':torch.tensor(X), 'y':torch.tensor(y), 'family':torch.tensor(f)}\n",
        "\n",
        "    return pack('train'), pack('val'), pack('test'), scaler\n",
        "\n",
        "# ---------------- Models (your encoder/decoder + small constraintor) ----------------\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, d, drop_path=0.0):\n",
        "        super().__init__(); self.ln1=nn.LayerNorm(d); self.fc1=nn.Linear(d,d); self.ln2=nn.LayerNorm(d); self.fc2=nn.Linear(d,d); self.drop_path=float(drop_path)\n",
        "    def forward(self,x):\n",
        "        h=self.fc1(F.gelu(self.ln1(x))); h=self.fc2(F.gelu(self.ln2(h)))\n",
        "        if self.training and self.drop_path>0 and torch.rand(())<self.drop_path: return x\n",
        "        return x+0.5*h\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_dim, hid=128, depth=1, p_drop=0.1, bottleneck=64, feat_drop=0.0):\n",
        "        super().__init__(); self.inp=nn.Linear(in_dim,hid); self.feat_drop=feat_drop\n",
        "        self.blocks=nn.ModuleList([ResBlock(hid,drop_path=0.00) for _ in range(depth)]); self.dropout=nn.Dropout(p_drop); self.out=nn.Linear(hid,bottleneck)\n",
        "    def forward(self,x):\n",
        "        if self.training and getattr(self,'feat_drop',0.0)>0:\n",
        "            m=torch.rand_like(x)>self.feat_drop; x=x*m\n",
        "        h=F.gelu(self.inp(x))\n",
        "        for b in self.blocks: h=b(h)\n",
        "        return self.out(self.dropout(h))\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, bottleneck=64, hid=128, out_dim=32, depth=0):\n",
        "        super().__init__(); self.fc=nn.Linear(bottleneck,hid); self.blocks=nn.ModuleList([ResBlock(hid) for _ in range(depth)]); self.out=nn.Linear(hid,out_dim)\n",
        "    def forward(self,z):\n",
        "        h=F.gelu(self.fc(z))\n",
        "        for b in self.blocks: h=b(h)\n",
        "        return self.out(h)\n",
        "\n",
        "class FeatureConstraintor(nn.Module):\n",
        "    def __init__(self, in_dim, hid=128):\n",
        "        super().__init__(); self.net=nn.Sequential(nn.Linear(in_dim,hid), nn.BatchNorm1d(hid), nn.ReLU(True), nn.Linear(hid,in_dim))\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "# ---------------- Losses & simple Gaussian scorer ----------------\n",
        "\n",
        "def occ_loss(res_c: torch.Tensor, res_init: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    y=y.float(); norm=torch.linalg.norm(res_c,dim=1); loss_norm=torch.sqrt(1.0+norm*norm)-1.0; loss_abn=torch.sum((res_c-res_init)**2,dim=1)\n",
        "    return ((1-y)*loss_norm + y*loss_abn).mean()\n",
        "\n",
        "class RunningDiagGaussian:\n",
        "    def __init__(self, dim:int, eps:float=1e-5): self.dim=dim; self.eps=eps; self.n=0; self.mean=torch.zeros(dim); self.M2=torch.zeros(dim)\n",
        "    def to(self, device): self.mean=self.mean.to(device); self.M2=self.M2.to(device); return self\n",
        "    @torch.no_grad()\n",
        "    def update(self, x: torch.Tensor):\n",
        "        for i in range(x.size(0)):\n",
        "            xi=x[i]; self.n+=1; d=xi-self.mean; self.mean+=d/self.n; self.M2+=d*(xi-self.mean)\n",
        "    def var(self):\n",
        "        if self.n<2: return torch.ones_like(self.mean)\n",
        "        return self.M2 / (self.n-1)\n",
        "    def logp(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        var = self.var() + self.eps\n",
        "        const = 0.5*self.dim*math.log(2*math.pi)\n",
        "        return - (const + 0.5*( ((x-self.mean)**2/var).sum(dim=1) + torch.log(var).sum() ))\n",
        "\n",
        "def compute_thr_at_fixed_fpr(y_true: np.ndarray, scores: np.ndarray, fpr: float = 0.05) -> float:\n",
        "    normals=scores[y_true==0]\n",
        "    if len(normals)==0: return float(np.percentile(scores, 100*(1-fpr)))\n",
        "    return float(np.quantile(normals, 1-fpr))\n",
        "\n",
        "def eval_block(y_true: np.ndarray, scores: np.ndarray, thr: Optional[float]=None):\n",
        "    auc = roc_auc_score(y_true, scores) if len(np.unique(y_true))>1 else float('nan')\n",
        "    ap = average_precision_score(y_true, scores) if len(np.unique(y_true))>1 else float('nan')\n",
        "    if thr is None:\n",
        "        p,r,t = precision_recall_curve(y_true, scores)\n",
        "        f1s = 2*p*r/(p+r+1e-9); i = np.nanargmax(f1s); thr = t[i] if i<len(t) else np.inf\n",
        "    y_pred = (scores>=thr).astype(int)\n",
        "    tp=int(((y_pred==1)&(y_true==1)).sum()); fp=int(((y_pred==1)&(y_true==0)).sum()); fn=int(((y_pred==0)&(y_true==1)).sum()); tn=int(((y_pred==0)&(y_true==0)).sum())\n",
        "    prec = tp/(tp+fp+1e-9); rec=tp/(tp+fn+1e-9); f1=2*prec*rec/(prec+rec+1e-9); acc=(tp+tn)/max(1,(tp+tn+fp+fn))\n",
        "    return {'threshold':float(thr),'auc_roc':float(auc),'ap':float(ap),'precision':float(prec),'recall':float(rec),'f1':float(f1),'accuracy':float(acc)}\n",
        "\n",
        "def per_family_report(y: np.ndarray, s: np.ndarray, fam: np.ndarray, fam_names: List[str], thr: float) -> List[str]:\n",
        "    lines=[]\n",
        "    for i,name in enumerate(fam_names):\n",
        "        m=(fam==i);\n",
        "        if not m.any(): continue\n",
        "        yi,si=y[m],s[m]\n",
        "        mtr=eval_block(yi,si,thr)\n",
        "        n=yi.size; pos=int((yi==1).sum()); neg=n-pos\n",
        "        lines.append(f\"  [{i:02d}] {name} | N={n} (+:{pos}/-:{neg}) | AUC={mtr['auc_roc']:.3f} AP={mtr['ap']:.3f} F1={mtr['f1']:.3f} P={mtr['precision']:.3f} R={mtr['recall']:.3f} Acc={mtr['accuracy']:.3f}\")\n",
        "    return lines\n",
        "\n",
        "# ---------------- Train ----------------\n",
        "\n",
        "def train(args):\n",
        "    device='cuda' if torch.cuda.is_available() and not args.cpu else 'cpu'\n",
        "\n",
        "    files=discover_files_by_category(args.data_dir)\n",
        "    fam_names=sorted(files.keys())\n",
        "    print(\"Discovered families:\");\n",
        "    for k in fam_names: print(\"  \",k)\n",
        "\n",
        "    # probe schema on first file\n",
        "    sample_df=pd.read_csv(files[fam_names[0]], nrows=2000)\n",
        "    label_col=infer_label_column(sample_df)\n",
        "    numeric_cols = topk_variance_columns(sample_df, exclude=[label_col], k=args.feat_topk)\n",
        "    print(f\"Numeric dim (capped): {len(numeric_cols)} | Label: {label_col}\")\n",
        "\n",
        "    policy=SplitPolicy(train_frac=args.train_frac, val_frac=args.val_frac, seed=args.seed)\n",
        "\n",
        "    # build tiny, fixed-size datasets covering all families\n",
        "    train_pack, val_pack, test_pack, scaler = build_micro_pilot(\n",
        "        files, fam_names, numeric_cols, label_col, policy, args.chunk_rows,\n",
        "        per_fam_norm=args.min_per_class, per_fam_anom=args.min_per_class)\n",
        "\n",
        "    class PackDS(Dataset):\n",
        "        def __init__(self,p): self.X=p['X']; self.y=p['y']; self.f=p['family']\n",
        "        def __len__(self): return self.X.size(0)\n",
        "        def __getitem__(self,i): return {'x':self.X[i],'y':self.y[i],'fam':self.f[i]}\n",
        "\n",
        "    ds_tr, ds_va, ds_te = PackDS(train_pack), PackDS(val_pack), PackDS(test_pack)\n",
        "    loader_tr=DataLoader(ds_tr, batch_size=args.batch, shuffle=True, num_workers=0)\n",
        "    loader_va=DataLoader(ds_va, batch_size=args.batch, shuffle=False, num_workers=0)\n",
        "    loader_te=DataLoader(ds_te, batch_size=args.batch, shuffle=False, num_workers=0)\n",
        "\n",
        "    D=len(numeric_cols)\n",
        "    enc=Encoder(D, hid=args.hid, depth=args.depth, p_drop=args.p_drop, bottleneck=args.bottleneck, feat_drop=args.feat_drop).to(device)\n",
        "    dec=Decoder(bottleneck=args.bottleneck, hid=args.hid, out_dim=D, depth=args.dec_depth).to(device)\n",
        "    con=FeatureConstraintor(in_dim=args.bottleneck, hid=max(64,args.bottleneck)).to(device)\n",
        "\n",
        "    # optional AE (defaults to 0 epoch)\n",
        "    if args.epochs_ae>0:\n",
        "        opt_ae=torch.optim.Adam(list(enc.parameters())+list(dec.parameters()), lr=args.lr_ae, weight_decay=5e-4)\n",
        "        for ep in range(1,args.epochs_ae+1):\n",
        "            enc.train(); dec.train(); tot=0.0; n=0\n",
        "            for b in tqdm(loader_tr, desc=f\"[AE] {ep}/{args.epochs_ae}\"):\n",
        "                x=b['x'].to(device); y=b['y'].to(device); m=(y==0)\n",
        "                if not m.any(): continue\n",
        "                xn=x[m]; z=enc(xn); xhat=dec(z); loss=F.mse_loss(xhat,xn)\n",
        "                opt_ae.zero_grad(set_to_none=True); loss.backward(); opt_ae.step(); tot+=loss.item()*xn.size(0); n+=xn.size(0)\n",
        "            print(f\"[AE] mean MSE={tot/max(1,n):.6f}\")\n",
        "    else:\n",
        "        print(\"[AE] Skipped (set --epochs_ae 1 if you want a quick pretrain)\")\n",
        "\n",
        "    if not args.finetune_encoder:\n",
        "        for p in enc.parameters(): p.requires_grad_(False)\n",
        "        enc.eval(); print(\"[Enc] Frozen.\")\n",
        "\n",
        "    # Build centroids from train normals\n",
        "    centroids=[torch.zeros(args.bottleneck, device=device) for _ in fam_names]\n",
        "    counts=[0 for _ in fam_names]\n",
        "    enc.eval()\n",
        "    with torch.no_grad():\n",
        "        for b in DataLoader(ds_tr, batch_size=args.batch, shuffle=False):\n",
        "            x=b['x'].to(device); y=b['y'].to(device); f=b['fam'].to(device); m=(y==0)\n",
        "            if not m.any(): continue\n",
        "            z=enc(x[m])\n",
        "            for zi,fi in zip(z, f[m].tolist()):\n",
        "                counts[fi]+=1; centroids[fi] = centroids[fi] + (zi - centroids[fi]) / counts[fi]\n",
        "    print(\"Centroids built.\")\n",
        "\n",
        "    def z_ref_for_batch(z, fam):\n",
        "        out=torch.zeros_like(z)\n",
        "        for fid in fam.unique().tolist():\n",
        "            out[fam==fid]=centroids[fid]\n",
        "        return out\n",
        "\n",
        "    gauss=RunningDiagGaussian(dim=args.bottleneck).to(device)\n",
        "\n",
        "    # Quick training loop (few epochs on tiny data)\n",
        "    params=list(con.parameters()) + (list(enc.parameters()) if args.finetune_encoder else [])\n",
        "    opt=torch.optim.Adam(params, lr=args.lr, weight_decay=5e-4)\n",
        "\n",
        "    for ep in range(1, args.epochs+1):\n",
        "        enc.train(args.finetune_encoder); con.train(); tot={'occ':0.0,'tot':0.0,'n':0}\n",
        "        for b in tqdm(loader_tr, desc=f\"[Mini-ResAD] {ep}/{args.epochs}\"):\n",
        "            x=b['x'].to(device); y=b['y'].to(device); f=b['fam'].to(device)\n",
        "            z=enc(x); zc=z - z_ref_for_batch(z,f); rc=con(zc)\n",
        "            with torch.no_grad(): gauss.update(rc[y==0])\n",
        "            logp = gauss.logp(rc)\n",
        "            L_occ=occ_loss(rc, zc, y)\n",
        "            # tiny boundary push (optional); using mean-std at each iter\n",
        "            bn = logp[y==0].mean() - args.bn_k*logp[y==0].std(unbiased=False)\n",
        "            L_bg = F.relu(logp - (bn - args.tau)).mean()\n",
        "            loss = L_occ + args.lambda_bg*L_bg\n",
        "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
        "            B=x.size(0); tot['occ']+=L_occ.item()*B; tot['tot']+=loss.item()*B; tot['n']+=B\n",
        "        print(f\"[Train] ep={ep} L={tot['tot']/tot['n']:.5f} (occ={tot['occ']/tot['n']:.5f})\")\n",
        "\n",
        "        # Validation\n",
        "        enc.eval(); con.eval(); scores=[]; labels=[]; fams=[]\n",
        "        with torch.no_grad():\n",
        "            for b in loader_va:\n",
        "                x=b['x'].to(device); y=b['y'].to(device); f=b['fam'].to(device)\n",
        "                z=enc(x); zc=z - z_ref_for_batch(z,f); rc=con(zc); s = -gauss.logp(rc)\n",
        "                scores.append(s.cpu().numpy()); labels.append(y.cpu().numpy()); fams.append(f.cpu().numpy())\n",
        "        if scores:\n",
        "            yv=np.concatenate(labels); sv=np.concatenate(scores); fv=np.concatenate(fams)\n",
        "            thr=compute_thr_at_fixed_fpr(yv, sv, fpr=args.fixed_fpr)\n",
        "            mtr=eval_block(yv, sv, thr)\n",
        "            print(f\"[VAL] AUC={mtr['auc_roc']:.3f} AP={mtr['ap']:.3f} F1={mtr['f1']:.3f} Thr@FPR{args.fixed_fpr:.2f}={thr:.6f}\")\n",
        "            print(\"[VAL][Per-class] (global thr) →\");\n",
        "            for line in per_family_report(yv, sv, fv, fam_names, thr): print(line)\n",
        "\n",
        "    # Test (overall + per-class)\n",
        "    enc.eval(); con.eval(); scores=[]; labels=[]; fams=[]\n",
        "    with torch.no_grad():\n",
        "        for b in loader_te:\n",
        "            x=b['x'].to(device); y=b['y'].to(device); f=b['fam'].to(device)\n",
        "            z=enc(x); zc=z - z_ref_for_batch(z,f); rc=con(zc); s = -gauss.logp(rc)\n",
        "            scores.append(s.cpu().numpy()); labels.append(y.cpu().numpy()); fams.append(f.cpu().numpy())\n",
        "    if scores:\n",
        "        yt=np.concatenate(labels); st=np.concatenate(scores); ft=np.concatenate(fams)\n",
        "        thr=compute_thr_at_fixed_fpr(yt, st, fpr=args.fixed_fpr)\n",
        "        mtr=eval_block(yt, st, thr)\n",
        "        print(f\"\\n[TEST] Thr@FPR{args.fixed_fpr:.2f}={thr:.6f} | AUC={mtr['auc_roc']:.3f} AP={mtr['ap']:.3f} F1={mtr['f1']:.3f} Prec={mtr['precision']:.3f} Rec={mtr['recall']:.3f} Acc={mtr['accuracy']:.3f}\")\n",
        "        print(\"[TEST][Per-class] (global thr) →\")\n",
        "        for line in per_family_report(yt, st, ft, fam_names, thr): print(line)\n",
        "\n",
        "# ---------------- CLI ----------------\n",
        "\n",
        "def get_args(argv=None):\n",
        "    p=argparse.ArgumentParser(description=\"ResAD micro pilot (all families, tiny per-class sample)\")\n",
        "    p.add_argument('--data_dir', type=str, default=None)\n",
        "    p.add_argument('--seed', type=int, default=42)\n",
        "    p.add_argument('--cpu', action='store_true')\n",
        "\n",
        "    p.add_argument('--train_frac', type=float, default=2/3)\n",
        "    p.add_argument('--val_frac', type=float, default=1/6)\n",
        "    p.add_argument('--chunk_rows', type=int, default=10000)\n",
        "    p.add_argument('--batch', type=int, default=512)\n",
        "\n",
        "    # tiny sample size per family per split\n",
        "    p.add_argument('--min_per_class', type=int, default=1000, help='per family per split for normal and anomaly each')\n",
        "    # feature cap\n",
        "    p.add_argument('--feat_topk', type=int, default=76)\n",
        "\n",
        "    # model\n",
        "    p.add_argument('--bottleneck', type=int, default=64)\n",
        "    p.add_argument('--hid', type=int, default=128)\n",
        "    p.add_argument('--depth', type=int, default=1)\n",
        "    p.add_argument('--dec_depth', type=int, default=0)\n",
        "    p.add_argument('--p_drop', type=float, default=0.05)\n",
        "    p.add_argument('--feat_drop', type=float, default=0.0)\n",
        "\n",
        "    # training\n",
        "    p.add_argument('--epochs_ae', type=int, default=3)\n",
        "    p.add_argument('--epochs', type=int, default=5)\n",
        "    p.add_argument('--lr_ae', type=float, default=1e-3)\n",
        "    p.add_argument('--lr', type=float, default=1e-3)\n",
        "    p.add_argument('--finetune_encoder', action='store_true')\n",
        "    p.add_argument('--lambda_bg', type=float, default=0.5)\n",
        "    p.add_argument('--tau', type=float, default=0.1)\n",
        "    p.add_argument('--bn_k', type=float, default=1.0)\n",
        "    p.add_argument('--fixed_fpr', type=float, default=0.05)\n",
        "\n",
        "    # Swallow unknown Jupyter flags unless argv is provided\n",
        "    if argv is None:\n",
        "        args, _ = p.parse_known_args()\n",
        "    else:\n",
        "        args = p.parse_args(argv)\n",
        "\n",
        "    if args.data_dir is None:\n",
        "        env=os.getenv('DATA_DIR') or os.getenv('CSE_CIC_IDS_DATA_DIR')\n",
        "        guess=\"/content/gdrive/MyDrive/DATASETS/CSE_CIC_IDS_ALL_DATA/CSE-CIC-IDS2018/Processed/\"\n",
        "        path = env if env and os.path.isdir(env) else (guess if os.path.isdir(guess) else None)\n",
        "        if path is None: raise SystemExit(\"Please provide --data_dir or set DATA_DIR env var\")\n",
        "        args.data_dir = path\n",
        "        print(f\"[INFO] auto-detected data_dir={path}\")\n",
        "    return args\n",
        "\n",
        "if __name__=='__main__':\n",
        "    args=get_args()\n",
        "    random.seed(args.seed); np.random.seed(args.seed); torch.manual_seed(args.seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(args.seed)\n",
        "    train(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhR91tbbvjMu",
        "outputId": "c7b0fb26-1bae-430c-d58b-e92b5c265c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] auto-detected data_dir=/content/gdrive/MyDrive/DATASETS/CSE_CIC_IDS_ALL_DATA/CSE-CIC-IDS2018/Processed/\n",
            "Discovered families:\n",
            "   Bot\n",
            "   Brute Force::Web\n",
            "   Brute Force::XSS\n",
            "   DDOS attack::HOIC\n",
            "   DDOS attack::LOIC-UDP\n",
            "   DDoS attacks::LOIC-HTTP\n",
            "   DoS attacks::GoldenEye\n",
            "   DoS attacks::Hulk\n",
            "   DoS attacks::SlowHTTPTest\n",
            "   DoS attacks::Slowloris\n",
            "   FTP::BruteForce\n",
            "   Infilteration\n",
            "   SQL Injection\n",
            "   SSH::Bruteforce\n",
            "Numeric dim (capped): 76 | Label: Label\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[AE] 1/3: 100%|██████████| 47/47 [00:00<00:00, 136.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] mean MSE=0.605230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[AE] 2/3: 100%|██████████| 47/47 [00:00<00:00, 143.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] mean MSE=0.271079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[AE] 3/3: 100%|██████████| 47/47 [00:00<00:00, 144.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] mean MSE=0.171057\n",
            "[Enc] Frozen.\n",
            "Centroids built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Mini-ResAD] 1/5: 100%|██████████| 47/47 [00:01<00:00, 43.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] ep=1 L=39.44205 (occ=27.21350)\n",
            "[VAL] AUC=0.852 AP=0.872 F1=0.801 Thr@FPR0.05=15.789808\n",
            "[VAL][Per-class] (global thr) →\n",
            "  [00] Bot | N=2000 (+:1000/-:1000) | AUC=0.659 AP=0.560 F1=0.000 P=0.000 R=0.000 Acc=0.484\n",
            "  [01] Brute Force::Web | N=366 (+:114/-:252) | AUC=0.736 AP=0.560 F1=0.373 P=0.778 R=0.246 Acc=0.743\n",
            "  [02] Brute Force::XSS | N=131 (+:51/-:80) | AUC=0.717 AP=0.501 F1=0.000 P=0.000 R=0.000 Acc=0.588\n",
            "  [03] DDOS attack::HOIC | N=2000 (+:1000/-:1000) | AUC=0.950 AP=0.919 F1=0.637 P=0.934 R=0.483 Acc=0.725\n",
            "  [04] DDOS attack::LOIC-UDP | N=1002 (+:309/-:693) | AUC=0.991 AP=0.942 F1=0.775 P=0.636 R=0.994 Acc=0.822\n",
            "  [05] DDoS attacks::LOIC-HTTP | N=2000 (+:1000/-:1000) | AUC=0.987 AP=0.947 F1=0.988 P=0.977 R=1.000 Acc=0.988\n",
            "  [06] DoS attacks::GoldenEye | N=2000 (+:1000/-:1000) | AUC=0.964 AP=0.921 F1=0.964 P=0.967 R=0.962 Acc=0.965\n",
            "  [07] DoS attacks::Hulk | N=2000 (+:1000/-:1000) | AUC=0.966 AP=0.918 F1=0.958 P=0.921 R=0.999 Acc=0.957\n",
            "  [08] DoS attacks::SlowHTTPTest | N=2000 (+:1000/-:1000) | AUC=1.000 AP=1.000 F1=0.982 P=0.964 R=1.000 Acc=0.982\n",
            "  [09] DoS attacks::Slowloris | N=2000 (+:1000/-:1000) | AUC=0.796 AP=0.822 F1=0.720 P=0.965 R=0.574 Acc=0.776\n",
            "  [10] FTP::BruteForce | N=2000 (+:1000/-:1000) | AUC=0.998 AP=0.997 F1=0.980 P=0.962 R=1.000 Acc=0.980\n",
            "  [11] Infilteration | N=2000 (+:1000/-:1000) | AUC=0.527 AP=0.529 F1=0.029 P=0.682 R=0.015 Acc=0.504\n",
            "  [12] SQL Injection | N=43 (+:15/-:28) | AUC=0.560 AP=0.411 F1=0.500 P=0.471 R=0.533 Acc=0.628\n",
            "  [13] SSH::Bruteforce | N=2000 (+:1000/-:1000) | AUC=0.979 AP=0.919 F1=0.979 P=0.959 R=1.000 Acc=0.979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Mini-ResAD] 2/5: 100%|██████████| 47/47 [00:01<00:00, 43.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] ep=2 L=21.30604 (occ=17.50270)\n",
            "[VAL] AUC=0.887 AP=0.910 F1=0.810 Thr@FPR0.05=16.589989\n",
            "[VAL][Per-class] (global thr) →\n",
            "  [00] Bot | N=2000 (+:1000/-:1000) | AUC=0.845 AP=0.697 F1=0.000 P=0.000 R=0.000 Acc=0.480\n",
            "  [01] Brute Force::Web | N=366 (+:114/-:252) | AUC=0.837 AP=0.647 F1=0.588 P=0.893 R=0.439 Acc=0.809\n",
            "  [02] Brute Force::XSS | N=131 (+:51/-:80) | AUC=0.669 AP=0.472 F1=0.000 P=0.000 R=0.000 Acc=0.588\n",
            "  [03] DDOS attack::HOIC | N=2000 (+:1000/-:1000) | AUC=0.970 AP=0.950 F1=0.636 P=0.931 R=0.483 Acc=0.724\n",
            "  [04] DDOS attack::LOIC-UDP | N=1002 (+:309/-:693) | AUC=0.998 AP=0.982 F1=0.820 P=0.694 R=1.000 Acc=0.864\n",
            "  [05] DDoS attacks::LOIC-HTTP | N=2000 (+:1000/-:1000) | AUC=0.999 AP=0.994 F1=0.980 P=0.962 R=1.000 Acc=0.980\n",
            "  [06] DoS attacks::GoldenEye | N=2000 (+:1000/-:1000) | AUC=0.960 AP=0.934 F1=0.960 P=0.960 R=0.960 Acc=0.960\n",
            "  [07] DoS attacks::Hulk | N=2000 (+:1000/-:1000) | AUC=0.964 AP=0.933 F1=0.967 P=0.936 R=1.000 Acc=0.966\n",
            "  [08] DoS attacks::SlowHTTPTest | N=2000 (+:1000/-:1000) | AUC=1.000 AP=1.000 F1=0.986 P=0.973 R=1.000 Acc=0.986\n",
            "  [09] DoS attacks::Slowloris | N=2000 (+:1000/-:1000) | AUC=0.825 AP=0.879 F1=0.809 P=0.964 R=0.697 Acc=0.836\n",
            "  [10] FTP::BruteForce | N=2000 (+:1000/-:1000) | AUC=0.998 AP=0.997 F1=0.987 P=0.974 R=1.000 Acc=0.987\n",
            "  [11] Infilteration | N=2000 (+:1000/-:1000) | AUC=0.559 AP=0.585 F1=0.028 P=0.875 R=0.014 Acc=0.506\n",
            "  [12] SQL Injection | N=43 (+:15/-:28) | AUC=0.640 AP=0.442 F1=0.545 P=0.500 R=0.600 Acc=0.651\n",
            "  [13] SSH::Bruteforce | N=2000 (+:1000/-:1000) | AUC=0.982 AP=0.936 F1=0.956 P=0.915 R=1.000 Acc=0.954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Mini-ResAD] 3/5: 100%|██████████| 47/47 [00:01<00:00, 39.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] ep=3 L=16.68660 (occ=13.12713)\n",
            "[VAL] AUC=0.890 AP=0.920 F1=0.843 Thr@FPR0.05=22.339529\n",
            "[VAL][Per-class] (global thr) →\n",
            "  [00] Bot | N=2000 (+:1000/-:1000) | AUC=0.899 AP=0.797 F1=0.638 P=0.905 R=0.493 Acc=0.721\n",
            "  [01] Brute Force::Web | N=366 (+:114/-:252) | AUC=0.842 AP=0.679 F1=0.588 P=0.893 R=0.439 Acc=0.809\n",
            "  [02] Brute Force::XSS | N=131 (+:51/-:80) | AUC=0.779 AP=0.679 F1=0.286 P=0.750 R=0.176 Acc=0.656\n",
            "  [03] DDOS attack::HOIC | N=2000 (+:1000/-:1000) | AUC=0.977 AP=0.969 F1=0.639 P=0.945 R=0.483 Acc=0.728\n",
            "  [04] DDOS attack::LOIC-UDP | N=1002 (+:309/-:693) | AUC=1.000 AP=1.000 F1=0.826 P=0.704 R=1.000 Acc=0.870\n",
            "  [05] DDoS attacks::LOIC-HTTP | N=2000 (+:1000/-:1000) | AUC=0.999 AP=0.994 F1=0.982 P=0.965 R=1.000 Acc=0.982\n",
            "  [06] DoS attacks::GoldenEye | N=2000 (+:1000/-:1000) | AUC=0.957 AP=0.943 F1=0.955 P=0.950 R=0.960 Acc=0.955\n",
            "  [07] DoS attacks::Hulk | N=2000 (+:1000/-:1000) | AUC=0.986 AP=0.976 F1=0.970 P=0.943 R=1.000 Acc=0.970\n",
            "  [08] DoS attacks::SlowHTTPTest | N=2000 (+:1000/-:1000) | AUC=1.000 AP=1.000 F1=0.985 P=0.970 R=1.000 Acc=0.985\n",
            "  [09] DoS attacks::Slowloris | N=2000 (+:1000/-:1000) | AUC=0.811 AP=0.873 F1=0.808 P=0.961 R=0.697 Acc=0.835\n",
            "  [10] FTP::BruteForce | N=2000 (+:1000/-:1000) | AUC=0.999 AP=0.998 F1=0.986 P=0.973 R=1.000 Acc=0.986\n",
            "  [11] Infilteration | N=2000 (+:1000/-:1000) | AUC=0.576 AP=0.597 F1=0.058 P=0.968 R=0.030 Acc=0.514\n",
            "  [12] SQL Injection | N=43 (+:15/-:28) | AUC=0.717 AP=0.544 F1=0.516 P=0.500 R=0.533 Acc=0.651\n",
            "  [13] SSH::Bruteforce | N=2000 (+:1000/-:1000) | AUC=0.988 AP=0.949 F1=0.957 P=0.917 R=1.000 Acc=0.955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Mini-ResAD] 4/5: 100%|██████████| 47/47 [00:01<00:00, 30.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] ep=4 L=14.20088 (occ=10.84755)\n",
            "[VAL] AUC=0.887 AP=0.923 F1=0.844 Thr@FPR0.05=24.865425\n",
            "[VAL][Per-class] (global thr) →\n",
            "  [00] Bot | N=2000 (+:1000/-:1000) | AUC=0.932 AP=0.910 F1=0.646 P=0.927 R=0.496 Acc=0.729\n",
            "  [01] Brute Force::Web | N=366 (+:114/-:252) | AUC=0.863 AP=0.706 F1=0.578 P=0.847 R=0.439 Acc=0.801\n",
            "  [02] Brute Force::XSS | N=131 (+:51/-:80) | AUC=0.697 AP=0.479 F1=0.000 P=0.000 R=0.000 Acc=0.565\n",
            "  [03] DDOS attack::HOIC | N=2000 (+:1000/-:1000) | AUC=0.981 AP=0.975 F1=0.637 P=0.936 R=0.483 Acc=0.725\n",
            "  [04] DDOS attack::LOIC-UDP | N=1002 (+:309/-:693) | AUC=1.000 AP=0.999 F1=0.801 P=0.667 R=1.000 Acc=0.846\n",
            "  [05] DDoS attacks::LOIC-HTTP | N=2000 (+:1000/-:1000) | AUC=0.999 AP=0.994 F1=0.978 P=0.957 R=1.000 Acc=0.978\n",
            "  [06] DoS attacks::GoldenEye | N=2000 (+:1000/-:1000) | AUC=0.958 AP=0.959 F1=0.954 P=0.949 R=0.960 Acc=0.954\n",
            "  [07] DoS attacks::Hulk | N=2000 (+:1000/-:1000) | AUC=0.996 AP=0.989 F1=0.979 P=0.959 R=1.000 Acc=0.979\n",
            "  [08] DoS attacks::SlowHTTPTest | N=2000 (+:1000/-:1000) | AUC=1.000 AP=1.000 F1=0.987 P=0.975 R=1.000 Acc=0.987\n",
            "  [09] DoS attacks::Slowloris | N=2000 (+:1000/-:1000) | AUC=0.747 AP=0.854 F1=0.800 P=0.906 R=0.716 Acc=0.821\n",
            "  [10] FTP::BruteForce | N=2000 (+:1000/-:1000) | AUC=0.998 AP=0.997 F1=0.986 P=0.972 R=1.000 Acc=0.986\n",
            "  [11] Infilteration | N=2000 (+:1000/-:1000) | AUC=0.583 AP=0.595 F1=0.084 P=1.000 R=0.044 Acc=0.522\n",
            "  [12] SQL Injection | N=43 (+:15/-:28) | AUC=0.707 AP=0.536 F1=0.588 P=0.526 R=0.667 Acc=0.674\n",
            "  [13] SSH::Bruteforce | N=2000 (+:1000/-:1000) | AUC=0.989 AP=0.958 F1=0.983 P=0.967 R=1.000 Acc=0.983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Mini-ResAD] 5/5: 100%|██████████| 47/47 [00:01<00:00, 35.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] ep=5 L=12.47085 (occ=9.32526)\n",
            "[VAL] AUC=0.884 AP=0.922 F1=0.846 Thr@FPR0.05=26.342413\n",
            "[VAL][Per-class] (global thr) →\n",
            "  [00] Bot | N=2000 (+:1000/-:1000) | AUC=0.907 AP=0.891 F1=0.644 P=0.919 R=0.496 Acc=0.726\n",
            "  [01] Brute Force::Web | N=366 (+:114/-:252) | AUC=0.811 AP=0.734 F1=0.754 P=0.882 R=0.658 Acc=0.866\n",
            "  [02] Brute Force::XSS | N=131 (+:51/-:80) | AUC=0.743 AP=0.644 F1=0.000 P=0.000 R=0.000 Acc=0.580\n",
            "  [03] DDOS attack::HOIC | N=2000 (+:1000/-:1000) | AUC=0.949 AP=0.938 F1=0.634 P=0.924 R=0.483 Acc=0.722\n",
            "  [04] DDOS attack::LOIC-UDP | N=1002 (+:309/-:693) | AUC=1.000 AP=0.999 F1=0.843 P=0.729 R=1.000 Acc=0.885\n",
            "  [05] DDoS attacks::LOIC-HTTP | N=2000 (+:1000/-:1000) | AUC=0.999 AP=0.994 F1=0.973 P=0.948 R=1.000 Acc=0.973\n",
            "  [06] DoS attacks::GoldenEye | N=2000 (+:1000/-:1000) | AUC=0.961 AP=0.968 F1=0.949 P=0.938 R=0.960 Acc=0.949\n",
            "  [07] DoS attacks::Hulk | N=2000 (+:1000/-:1000) | AUC=0.993 AP=0.991 F1=0.964 P=0.930 R=1.000 Acc=0.963\n",
            "  [08] DoS attacks::SlowHTTPTest | N=2000 (+:1000/-:1000) | AUC=1.000 AP=1.000 F1=0.984 P=0.969 R=1.000 Acc=0.984\n",
            "  [09] DoS attacks::Slowloris | N=2000 (+:1000/-:1000) | AUC=0.729 AP=0.850 F1=0.810 P=0.934 R=0.716 Acc=0.833\n",
            "  [10] FTP::BruteForce | N=2000 (+:1000/-:1000) | AUC=1.000 AP=1.000 F1=0.986 P=0.973 R=1.000 Acc=0.986\n",
            "  [11] Infilteration | N=2000 (+:1000/-:1000) | AUC=0.598 AP=0.603 F1=0.084 P=0.978 R=0.044 Acc=0.521\n",
            "  [12] SQL Injection | N=43 (+:15/-:28) | AUC=0.762 AP=0.615 F1=0.706 P=0.632 R=0.800 Acc=0.767\n",
            "  [13] SSH::Bruteforce | N=2000 (+:1000/-:1000) | AUC=0.995 AP=0.977 F1=0.986 P=0.973 R=1.000 Acc=0.986\n",
            "\n",
            "[TEST] Thr@FPR0.05=26.741859 | AUC=0.887 AP=0.923 F1=0.849 Prec=0.936 Rec=0.777 Acc=0.866\n",
            "[TEST][Per-class] (global thr) →\n",
            "  [00] Bot | N=2000 (+:1000/-:1000) | AUC=0.888 AP=0.867 F1=0.610 P=0.870 R=0.470 Acc=0.700\n",
            "  [01] Brute Force::Web | N=336 (+:96/-:240) | AUC=0.840 AP=0.729 F1=0.734 P=0.849 R=0.646 Acc=0.866\n",
            "  [02] Brute Force::XSS | N=141 (+:45/-:96) | AUC=0.623 AP=0.463 F1=0.073 P=0.200 R=0.044 Acc=0.638\n",
            "  [03] DDOS attack::HOIC | N=2000 (+:1000/-:1000) | AUC=0.938 AP=0.927 F1=0.652 P=0.909 R=0.508 Acc=0.729\n",
            "  [04] DDOS attack::LOIC-UDP | N=971 (+:287/-:684) | AUC=0.999 AP=0.999 F1=0.840 P=0.725 R=1.000 Acc=0.888\n",
            "  [05] DDoS attacks::LOIC-HTTP | N=2000 (+:1000/-:1000) | AUC=1.000 AP=1.000 F1=0.979 P=0.960 R=1.000 Acc=0.979\n",
            "  [06] DoS attacks::GoldenEye | N=2000 (+:1000/-:1000) | AUC=0.970 AP=0.983 F1=0.965 P=0.961 R=0.969 Acc=0.965\n",
            "  [07] DoS attacks::Hulk | N=2000 (+:1000/-:1000) | AUC=0.991 AP=0.983 F1=0.970 P=0.942 R=1.000 Acc=0.969\n",
            "  [08] DoS attacks::SlowHTTPTest | N=2000 (+:1000/-:1000) | AUC=1.000 AP=1.000 F1=0.986 P=0.972 R=1.000 Acc=0.986\n",
            "  [09] DoS attacks::Slowloris | N=2000 (+:1000/-:1000) | AUC=0.750 AP=0.863 F1=0.828 P=0.940 R=0.739 Acc=0.846\n",
            "  [10] FTP::BruteForce | N=2000 (+:1000/-:1000) | AUC=1.000 AP=1.000 F1=0.978 P=0.958 R=1.000 Acc=0.978\n",
            "  [11] Infilteration | N=2000 (+:1000/-:1000) | AUC=0.579 AP=0.608 F1=0.118 P=0.969 R=0.063 Acc=0.530\n",
            "  [12] SQL Injection | N=53 (+:12/-:41) | AUC=0.809 AP=0.449 F1=0.600 P=0.500 R=0.750 Acc=0.774\n",
            "  [13] SSH::Bruteforce | N=2000 (+:1000/-:1000) | AUC=0.993 AP=0.971 F1=0.985 P=0.970 R=1.000 Acc=0.985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python resad_mini.py --data_dir \"$DATA_DIR\" \\\n",
        "  --min_per_class 5000 --feat_topk 76 --bottleneck 32 --hid 96 --epochs 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-tVyuWwxJHM",
        "outputId": "b78464db-9583-4cc2-b58a-8206d3b27088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/gdrive/MyDrive/DATASETS/CSE_CIC_IDS_ALL_DATA/CSE-CIC-IDS2018/resad_mini.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eR5s2jxNxJJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R_r17Gk7v4qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%python resad_tabular.py \\\n",
        "  --data_dir \"/content/gdrive/MyDrive/DATASETS/CSE_CIC_IDS_ALL_DATA/CSE-CIC-IDS2018/Processed/\" \\\n",
        "  --epochs_ae 10 --epochs 50 --batch 2048 \\\n",
        "  --bottleneck 128 --hid 256 --n_ref 256 --fixed_fpr 0.05\n"
      ],
      "metadata": {
        "id": "BAk0brMlv4ta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}